{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://pbs.twimg.com/profile_images/1901589840/logo-ifmg-betim.jpg\" width=\"700\"></img>\n",
    "<b>Engenharia de Controle e Automação - Redes neurais artificiais</b> - Prof. Leandro Freitas\n",
    "\n",
    "### Lucas Pevidor Reis - 0024855 - TP3\n",
    "\n",
    "# 1  Objetivo\n",
    "\n",
    "Fazer a implementação de uma rede neural multilayer perceptron (MLP) para resolver o problema XOR comentado nas primeiras aulas. Deve ser implementado um algoritmo de retropropagação (backpropagation) manualmente para fazer o ajustes dos pesos e dos bias para uma rede neural estruturada como a seguir:\n",
    "\n",
    "![MLP](https://i.imgur.com/K998YGR.png)\n",
    "\n",
    "\n",
    "# 2 Aproximação da função lógica XOR utilizando uma rede neural MLP\n",
    "\n",
    "## 2.1 Função  a ser estimada:\n",
    "\n",
    "A função lógica XOR recebe 2 entradas binárias e retorna 0 caso elas sejam iguais, ou 1 caso sejam diferentes, conforme mostrado na tabela abaixo.\n",
    "\n",
    "![XOR](https://i.imgur.com/wg4Ckhs.png)\n",
    "\n",
    "Visto que a função não é linearmente separável, é necessário utilizar uma rede neural de múltiplas camadas para realizar sua aproximação. Neste caso, será utilizada uma rede neural de 2 camadas.\n",
    "\n",
    "## 2.2 Implementação da rede neural\n",
    "\n",
    "A rede neural foi implementada utilizando classes. A classe neurônio já havia sido implementada nos últimos trabalhos. Foram feitas algumas pequenas modificações, que serão mostradas a seguir. Também foram implementadas as classes Camada e Rede.\n",
    "\n",
    "### Classe neurônio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class Neuronio:\n",
    "\n",
    "    #Função de inicialização da classe\n",
    "    def __init__(self, n_entradas, func_act, use_bias):\n",
    "        #Se optar por utilizar bias, será adicionada uma coluna no final dos vetores de entrada com valor unitário.\n",
    "        self.use_bias = use_bias        \n",
    "        if use_bias:\n",
    "            self.pesos = np.empty((n_entradas+1, 1), dtype=float)\n",
    "        else:\n",
    "            self.pesos = np.empty((n_entradas, 1), dtype=float)\n",
    "        self.novosPesos = np.array([])\n",
    "        self.pesosAnteriores = self.pesos\n",
    "\n",
    "        self.func_act = func_act\n",
    "        self.delta = 0        \n",
    "    \n",
    "    def reset(self):\n",
    "        #Fazer pesos randomizados\n",
    "        vetor = np.ones((len(self.pesos[:,0]), 1), dtype=float)\n",
    "        for i in range(len(vetor[:,0])):\n",
    "            vetor[i,0] = np.random.normal(scale=0.3)\n",
    "        self.pesos = vetor\n",
    "        self.pesosAnteriores = vetor\n",
    "        self.novosPesos = np.array([])\n",
    "    \n",
    "    @staticmethod\n",
    "    def fSigmoid(v):\n",
    "        saida = 1/(1+np.e**(-v))\n",
    "        return saida\n",
    "    \n",
    "    #Definição de entradas\n",
    "    #Função utilizada para configurar o vetor de entrada interno da classe, juntamente com normalização, adição\n",
    "    #da coluna de bias. Utilizar quando a média e desvpad para normalização ainda não foram definidos.\n",
    "    def set_entradas_ant(self, entradas, normalizar):        \n",
    "        if self.use_bias:\n",
    "            vetor_1 = np.zeros([len(entradas[:,0]), 1]) + 1\n",
    "            if normalizar:\n",
    "                self.normalizar(entradas, False)\n",
    "                self.vetor_entradas = np.append(self.entr_normalizada, vetor_1, axis=1)\n",
    "            else:\n",
    "                self.vetor_entradas = np.append(entradas, vetor_1, axis=1)\n",
    "        else:\n",
    "            if normalizar:                \n",
    "                self.vetor_entradas = self.normalizar(entradas, False)\n",
    "            else:\n",
    "                self.vetor_entradas = entradas\n",
    "    \n",
    "    #Definir entradas após treinamento da rede\n",
    "    #Função utilizada para configurar o vetor de entrada interno da classe, juntamente com normalização, adição\n",
    "    #da coluna de bias. Utilizar quando a média e desvpad para normalização ainda JÁ foram definidos.\n",
    "    def set_entradas_pós(self, entradas, normalizar):\n",
    "        if self.use_bias:\n",
    "            vetor_1 = np.zeros([len(entradas[:,0]), 1]) + 1\n",
    "            if normalizar:            \n",
    "                self.vetor_entradas = np.append(self.normalizar(entradas, True), vetor_1, axis=1)\n",
    "            else:\n",
    "                self.vetor_entradas = np.append(entradas, vetor_1, axis=1)\n",
    "        else:\n",
    "            if normalizar:\n",
    "                self.vetor_entradas = self.normalizar(entradas, True)\n",
    "            else:                    \n",
    "                self.vetor_entradas = entradas\n",
    "    \n",
    "    def normalizar(self, entradas, utilizar_parametros_anteriores):\n",
    "        if utilizar_parametros_anteriores == False:\n",
    "            #Encontrar média e desvio padrão a partir das entradas fornecidas.\n",
    "            #Executado antes do treinamento da rede\n",
    "            self.media = np.empty(len(entradas[0,:]), dtype=float)\n",
    "            self.desvpad = np.empty(len(entradas[0,:]), dtype=float)\n",
    "            self.entr_normalizada = np.empty((len(entradas[:,0]), len(entradas[0,:])), dtype=float)\n",
    "        \n",
    "            for i in range(0, len(entradas[0,:])):\n",
    "                self.media[i] = np.mean(entradas[:,i])\n",
    "                self.desvpad[i] = np.sqrt(np.var(entradas[:,i]))            \n",
    "                self.entr_normalizada[:,i] = (entradas[:,i] - self.media[i] ) / self.desvpad[i]\n",
    "            return self.entr_normalizada\n",
    "        else:\n",
    "            #Normaliza os dados de entrada fornecidos conforme a média e desvio padrão já encontrados anteriormente\n",
    "            entr_nova_normalizada = np.empty((len(entradas[:,0]), len(entradas[0,:])), dtype=float)\n",
    "            for i in range(0, len(entradas[0,:])):                \n",
    "                entr_nova_normalizada[:,i] = (entradas[:,i] - self.media[i] ) / self.desvpad[i]            \n",
    "            \n",
    "            return entr_nova_normalizada    \n",
    "        \n",
    "    def proc_saida(self, entradas, normalizado):\n",
    "        if not normalizado:\n",
    "            #Normalização das entradas utilizando média e desvpad já encontrados\n",
    "            entr_nova_normalizada = self.normalizar(entradas, True)\n",
    "        else:\n",
    "            entr_nova_normalizada = entradas\n",
    "        \n",
    "        v = np.dot(entr_nova_normalizada, self.pesos)\n",
    "        saida = np.empty([1,1])\n",
    "        if(self.func_act == 'Bipolar'):    \n",
    "            for i in range(len(v)):\n",
    "                if(v[i,:] > 0):\n",
    "                    saida = np.append(saida, [[1]], axis=0)\n",
    "                else:\n",
    "                    saida = np.append(saida, [[-1]], axis=0)\n",
    "            self.saida = saida[1:]\n",
    "            return saida[1:]\n",
    "        \n",
    "        elif(self.func_act == 'Linear'):            \n",
    "            self.saida = v\n",
    "            return saida\n",
    "\n",
    "        elif(self.func_act == 'Sigmoid'): \n",
    "            self.saida = self.fSigmoid(v)\n",
    "            return self.saida\n",
    "        \n",
    "    def treinar_rede(self, fator_aprendizado, saida_desejada, max_iterações):\n",
    "        redeTreinada = False\n",
    "        self.historicoAcertos = []\n",
    "        self.historicoPesos = np.empty([len(self.pesos), 1])\n",
    "        \n",
    "        for i in range(0, max_iterações):\n",
    "            if not redeTreinada:\n",
    "                flagAlteração = False\n",
    "                acertos = 0\n",
    "                for j in range(len(self.vetor_entradas[:,0])):\n",
    "                    #Testa as entradas até encontrar um erro\n",
    "                    #Caso encontrado, continua testando as outras entradas para obter a quantidade de acertos para\n",
    "                    #os pesos atuais. Em seguida atualiza os pesos e reinicia os testes.\n",
    "                    saida_proc = self.proc_saida([self.vetor_entradas[j,:]], True)[0,0]\n",
    "                    res = saida_desejada[j,0] - saida_proc\n",
    "                    if res == 0 and self.func_act=='Bipolar':\n",
    "                        acertos += 1\n",
    "                    else:\n",
    "                        if self.func_act == 'Linear' or self.func_act == 'Bipolar':\n",
    "                            deltaW = fator_aprendizado * res * self.vetor_entradas[j,:]\n",
    "                        elif self.func_act == 'Sigmoid':\n",
    "                            deltaW = fator_aprendizado * res * self.vetor_entradas[j,:] * saida_proc * (1 - saida_proc)\n",
    "                        flagAlteração = True                                                \n",
    "                        \n",
    "                        #Se a rede for bipolar, termina de checar a quantidade de acertos\n",
    "                        #antes de atualizar os pesos\n",
    "                        if self.func_act=='Bipolar':\n",
    "                            for k in range(j+1, len(self.vetor_entradas[:,0])):\n",
    "                                res = saida_desejada[k,0] - self.proc_saida([self.vetor_entradas[k,:]], True)[0,0]\n",
    "                                if res == 0:\n",
    "                                    acertos += 1\n",
    "                        \n",
    "                            #Salva os acertos e pesos\n",
    "                            self.historicoAcertos.append(acertos)\n",
    "                        self.historicoPesos = np.append(self.historicoPesos, self.pesos, axis=1)\n",
    "                        \n",
    "                        #Atualiza os pesos                               \n",
    "                        self.pesos += np.transpose([deltaW])\n",
    "                        if self.func_act == 'Bipolar':\n",
    "                            break\n",
    "                \n",
    "                if flagAlteração == False:\n",
    "                    redeTreinada = True\n",
    "                    self.historicoAcertos.append(acertos)\n",
    "                    self.historicoPesos = np.append(self.historicoPesos, self.pesos, axis=1)                \n",
    "        \n",
    "        self.historicoErros = []\n",
    "        for i in range(len(self.historicoAcertos)):            \n",
    "            self.historicoErros.append(len(self.vetor_entradas) - self.historicoAcertos[i])\n",
    "        \n",
    "        if redeTreinada == False:\n",
    "            if self.func_act=='Bipolar':\n",
    "                #Se não foi possível obter 100% de acerto durante as iterações\n",
    "                #encontra e define como pesos os que obtiveram mais acertos\n",
    "                indiceMax = self.historicoAcertos.index(max(self.historicoAcertos))                        \n",
    "                self.pesos = np.transpose([self.historicoPesos[:, indiceMax+1]])\n",
    "                print('Nº erros: {}'.format(len(self.vetor_entradas[:,0])-max(self.historicoAcertos)))\n",
    "\n",
    "    def trocarPesos(self):\n",
    "        self.pesosAnteriores = self.pesos\n",
    "        self.pesos = self.novosPesos\n",
    "        self.novosPesos = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relacionando a classe atual com implementação do neurônio realizada nos últimos trabalhos, foram realizadas as seguintes alterações:\n",
    "\n",
    "**Função init**\n",
    "~~~~python\n",
    "def __init__(self, n_entradas, func_act, use_bias):\n",
    "    #Se optar por utilizar bias, será adicionada uma coluna no final dos vetores de entrada com valor unitário.\n",
    "    self.use_bias = use_bias        \n",
    "    if use_bias:\n",
    "        self.pesos = np.empty((n_entradas+1, 1), dtype=float)\n",
    "    else:\n",
    "        self.pesos = np.empty((n_entradas, 1), dtype=float)\n",
    "    self.novosPesos = np.array([])\n",
    "    self.pesosAnteriores = self.pesos\n",
    "\n",
    "    self.func_act = func_act\n",
    "    self.delta = 0    \n",
    "~~~~\n",
    "\n",
    "Na função init foram adicionadas as variáveis *novosPesos*, *pesosAnteriores* e *delta*. Estas variáveis foram adicionadas para o cálculo do $\\delta$, utilizado no algoritmo de *backpropagation*.\n",
    "\n",
    "**Função reset**\n",
    "~~~~python\n",
    "def reset(self):\n",
    "    #Fazer pesos randomizados\n",
    "    vetor = np.ones((len(self.pesos[:,0]), 1), dtype=float)\n",
    "    for i in range(len(vetor[:,0])):\n",
    "        vetor[i,0] = np.random.normal(scale=0.3)\n",
    "    self.pesos = vetor\n",
    "    self.pesosAnteriores = vetor\n",
    "    self.novosPesos = np.array([])\n",
    "~~~~\n",
    "\n",
    "A função reset foi alterada para ao invés de configurar os pesos para 0, configurá-los para valores randomizados obtidos a partir de uma distribuição normal de escala 0,3.\n",
    "\n",
    "**Função fSigmoid**\n",
    "~~~~python\n",
    "@staticmethod\n",
    "def fSigmoid(v):\n",
    "    saida = 1/(1+np.e**(-v))\n",
    "    return saida\n",
    "~~~~\n",
    "\n",
    "A função de sigmóide logística que estava implementada diretamente no algoritmo de treinamento do neurônio foi trocada para um método estático, para permitir o acesso da mesma por fora da classe. O acesso é realizado pela classe *RedeNeural*, durante a execução do algoritmo de *backpropagation*.\n",
    "\n",
    "**Função trocarPesos**\n",
    "~~~~python\n",
    "def trocarPesos(self):\n",
    "    self.pesosAnteriores = self.pesos\n",
    "    self.pesos = self.novosPesos\n",
    "    self.novosPesos = np.array([])\n",
    "~~~~\n",
    "\n",
    "A função *trocarPesos* realiza a circulação dos pesos no neurônio, trocando o conteúdo da variável *pesos* para a *pesosAnteriores* e de *novosPesos* para *pesos*.\n",
    "\n",
    "### Classe Camada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Camada:\n",
    "    def __init__(self, n_entradas, n_neuronios, func_act):\n",
    "        self.n_neuronios = n_neuronios\n",
    "        self.n_entradas = n_entradas\n",
    "        self.func_act = func_act\n",
    "        self.saidas = np.zeros([n_neuronios, 1])\n",
    "        self.entradas = np.zeros([n_entradas, 1])\n",
    "\n",
    "        self.neuronios = []\n",
    "        for i in range(n_neuronios):\n",
    "            self.neuronios.append(Neuronio(n_entradas, func_act, use_bias=True))\n",
    "            self.neuronios[i].reset()\n",
    "    \n",
    "    def setEntradas(self):\n",
    "        for i in range(self.n_neuronios):\n",
    "            self.neuronios[i].set_entradas_ant(np.transpose(self.entradas), False)\n",
    "\n",
    "    def proc_saida(self):\n",
    "        self.setEntradas()\n",
    "        for i in range(self.n_neuronios):\n",
    "            self.saidas[i,0] = self.neuronios[i].proc_saida(self.neuronios[i].vetor_entradas, normalizado=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As camadas consistem em um conjunto de neurônios. Ela é definida utilizando os parâmetros:\n",
    "1. Número de entradas: Número de entradas de cada neurônio na camada\n",
    "1. Número de neurônios: Quantidade de neurônios presentes na camada\n",
    "1. Função de ativação: Função de ativação de cada neurônio\n",
    "\n",
    "As camadas possuem um vetor de dimensão $(m,1)$ para entradas e um vetor $(n,1)$ para saídas. Esses vetores são utilizados para interligar as camadas da rede neural. Os neurônios da camada são organizados em uma lista. Estes são instanciados sempre utilizando o *bias*, e logo em seguida são resetados, para que seus pesos sejam randomizados.\n",
    "\n",
    "A função *setEntradas* é responsável por ler os dados no vetor de entradas da camada e inserí-los nas entradas de cada neurônio, utilizando o método *Neuronios.set_entradas_ant*.\n",
    "\n",
    "A função *proc_saida* processa a saída de cada neurônio presente na camada e organiza o resultado no vetor de saídas da camada.\n",
    "\n",
    "### Classe RedeNeural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedeNeural:\n",
    "    def __init__(self, n_entradas, n_camadasOcultas, nn_camadasOcultas, n_saidas, func_act):\n",
    "        self.n_entradas = n_entradas\n",
    "        self.n_camadasOcultas = n_camadasOcultas\n",
    "        self.n_saidas = n_saidas\n",
    "        self.func_act=func_act\n",
    "        self.nn_camadasOcultas = nn_camadasOcultas\n",
    "        \n",
    "        self.listaEntradas = np.zeros([0,0]) #Lista com todas as entradas a serem processadas\n",
    "        self.entradas = np.zeros([n_entradas, 1]) #Vetor de entradas para jogar nas camadas\n",
    "        self.saidas = np.zeros([n_saidas, 1]) #Vetor de saída p/ receber das camadas\n",
    "        self.listaSaidas = []\n",
    "\n",
    "        self.camadas = []\n",
    "        #Primeira camada, 1 neurônio para cada entrada\n",
    "        c = 0\n",
    "        self.camadas.append(Camada(n_entradas, n_entradas, func_act))\n",
    "        self.camadas[c].entradas = self.entradas\n",
    "        c += 1\n",
    "        #Primeira camada oculta, n_entradas igual a nº de entradas\n",
    "        if n_camadasOcultas != 0:\n",
    "            self.camadas.append(Camada(n_entradas, nn_camadasOcultas, func_act))\n",
    "            self.camadas[c].entradas = self.camadas[c-1].saidas\n",
    "            c += 1\n",
    "            #Demais camadas ocultas\n",
    "            for i in range(n_camadasOcultas-1):\n",
    "                self.camadas.append(Camada(nn_camadasOcultas, nn_camadasOcultas, func_act))\n",
    "                self.camadas[c].entradas = self.camadas[c-1].saidas\n",
    "                c += 1\n",
    "            self.camadas.append(Camada(nn_camadasOcultas, n_saidas, func_act))\n",
    "        else:\n",
    "            self.camadas.append(Camada(n_entradas, n_saidas, func_act))\n",
    "        self.camadas[c].entradas = self.camadas[c-1].saidas\n",
    "        self.saidas = self.camadas[c].saidas\n",
    "\n",
    "        self.n_camadas = c+1\n",
    "    \n",
    "    def set_entradas_pós(self, entradas, normalizar):\n",
    "        if normalizar:           \n",
    "            vetorEntradas = self.normalizar(entradas, True)     \n",
    "            for i in range(len(vetorEntradas)):\n",
    "                self.entradas[i] = vetorEntradas[i]\n",
    "        else:\n",
    "            for i in range(len(entradas)):\n",
    "                self.entradas[i] = entradas[i]\n",
    "    \n",
    "    def normalizar(self, entradas, utilizar_parametros_anteriores):\n",
    "        if utilizar_parametros_anteriores == False:\n",
    "            #Encontrar média e desvio padrão a partir das entradas fornecidas.\n",
    "            #Executado antes do treinamento da rede\n",
    "            self.media = np.empty(len(entradas[0,:]), dtype=float)\n",
    "            self.desvpad = np.empty(len(entradas[0,:]), dtype=float)\n",
    "            self.entr_normalizada = np.empty((len(entradas[:,0]), len(entradas[0,:])), dtype=float)\n",
    "        \n",
    "            for i in range(0, len(entradas[0,:])):\n",
    "                self.media[i] = np.mean(entradas[:,i])\n",
    "                self.desvpad[i] = np.sqrt(np.var(entradas[:,i]))            \n",
    "                self.entr_normalizada[:,i] = (entradas[:,i] - self.media[i] ) / self.desvpad[i]\n",
    "            return self.entr_normalizada\n",
    "        else:\n",
    "            #Normaliza os dados de entrada fornecidos conforme a média e desvio padrão já encontrados anteriormente\n",
    "            entr_nova_normalizada = np.empty((len(entradas[:,0]), len(entradas[0,:])), dtype=float)\n",
    "            for i in range(0, len(entradas[0,:])):                \n",
    "                entr_nova_normalizada[:,i] = (entradas[:,i] - self.media[i] ) / self.desvpad[i]            \n",
    "\n",
    "            return entr_nova_normalizada\n",
    "\n",
    "    def proc_saidas(self, normalizar):        \n",
    "        self.listaSaidas = []\n",
    "        for k in range(len(self.listaEntradas)):\n",
    "            self.set_entradas_pós(np.transpose([self.listaEntradas[k]]), normalizar)\n",
    "            for i in range(self.n_camadas):\n",
    "                self.camadas[i].proc_saida()\n",
    "            self.listaSaidas.append(self.saidas.copy())\n",
    "\n",
    "    def proc_saida(self, indice, normalizar):\n",
    "        self.listaSaidas = []\n",
    "        self.set_entradas_pós(np.transpose([self.listaEntradas[indice]]), normalizar)\n",
    "        for i in range(self.n_camadas):\n",
    "            self.camadas[i].proc_saida()\n",
    "        self.listaSaidas.append(self.saidas.copy())\n",
    "\n",
    "    def treinar_rede(self, saidas_desejadas, tx_aprendizado_inicial, annealing, tx_momento, n_iteracoes):        \n",
    "        self.registroTreinamento = []\n",
    "        #primeiro passo: Processar a saída da rede com os dados atuais. A listaEntradas deve estar normalizada e configurada\n",
    "        for m in range(n_iteracoes):\n",
    "            print('-----------------------------------')\n",
    "            tx_aprendizado = tx_aprendizado_inicial/(1+(m/annealing))\n",
    "            print('eta = {} ; m={}'.format(tx_aprendizado, m))\n",
    "            for l in range(len(self.listaEntradas)):\n",
    "                self.proc_saida(l, False)\n",
    "                if l%25==0:\n",
    "                    print('[{}] - [{}]'.format(self.entradas[:,0], self.saidas))\n",
    "                self.registroTreinamento.append([self.entradas.copy(), self.saidas.copy()])\n",
    "                #cálculo do delta k e primeiro ajuste de pesos p/ última camada\n",
    "                for n in self.camadas[self.n_camadas-1].neuronios:\n",
    "                    n.delta = n.saida[0][0]*(1-n.saida[0][0])*(saidas_desejadas[l][0]-n.saida[0][0])\n",
    "                    momento = tx_momento * (n.pesos - n.pesosAnteriores)\n",
    "                    n.novosPesos = n.pesos + np.transpose([tx_aprendizado*n.delta*(np.append(self.camadas[self.n_camadas-2].saidas, np.array([[1]])))]) + momento\n",
    "                #cálculo do delta j e ajustes de pesos para camadas ocultas e primeira camada\n",
    "                for c in range(self.n_camadas-2, -1, -1):\n",
    "                    for k, n in enumerate(self.camadas[c].neuronios):\n",
    "                        #cálculo do somatório de wkdk da camada seguinte\n",
    "                        soma = 0\n",
    "                        for i, n2 in enumerate(self.camadas[c+1].neuronios):\n",
    "                            soma += n2.delta * n2.pesos[k, 0]\n",
    "                        #cálculo do deltaJ\n",
    "                        n.delta = n.saida[0][0]*(1-n.saida[0][0])*soma\n",
    "                        momento = tx_momento * (n.pesos - n.pesosAnteriores)\n",
    "                        n.novosPesos = n.pesos + np.transpose([tx_aprendizado*n.delta*(np.append(self.camadas[c].entradas, np.array([[1]])))]) + momento\n",
    "\n",
    "                #Troca os pesos dos neuronios\n",
    "                for i in range(len(self.camadas)):\n",
    "                    for n in self.camadas[i].neuronios:\n",
    "                        n.trocarPesos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para instanciar a *RedeNeural* devem ser fornecidos os parâmetros:\n",
    "1. Número de entradas da rede neural\n",
    "1. Número de camadas ocultas\n",
    "1. Número de neurônios de cada camada oculta\n",
    "1. Número de saídas\n",
    "1. Função de ativação\n",
    "\n",
    "Assim como as camadas, a rede neural tem vetores para armazenamento das entradas, denominado *entradas*, e saídas, denominado *saidas*. Além destes vetores, há 2 outros vetores para armazenamento de entradas e saídas:\n",
    "1. **listaEntradas**: Neste vetor são armazenados os conjuntos de entradas a serem processadas pela rede neural, com cada coluna no vetor representando uma entrada e cada linha representando um conjunto de entradas.\n",
    "1. **listaSaidas**: Armazena os conjuntos de saídas processadas pela rede neural, com cada coluna no vetor representando o resultado de uma saída, e cada linha representando o conjunto de saídas processados pela rede.\n",
    "\n",
    "Na rede neural as camadas são organizadas em uma lista denominada *camadas*. A primeira camada é instanciada de forma que a quantidade de neurônios seja igual à quantidade de entradas da rede neural. Após ser instanciada, seu vetor de entradas é referenciado ao vetor de entradas da rede neural.\n",
    "Em seguida são instanciadas as camadas ocultas. A primeira camada oculta é instanciada com número de entradas iguais ao número de saídas da primeira camada da rede. Suas entradas são referenciadas às saídas da camada anterior. As demais camadas ocultas são instanciadas com número de entradas iguais ao número de saídas das próprias camadas ocultas. Cada camada tem suas entradas referenciadas às saídas das camadas anteriores. Por fim, é instanciada a camada de saída da rede. Suas entradas são referenciadas às saídas da camada anterior e sua saída é referenciada ao vetor de saídas da rede neural.\n",
    "\n",
    "Uma estrutura simplificada da rede neural está representada na figura abaixo. Por motivos de organização não foram representadas todas as conexões dos neurônios. Cada neurônio está conectado à todas as saídas da camada anterior.\n",
    "![Strut](https://i.imgur.com/qQLPrjp.png)\n",
    "\n",
    "O método *set_entradas_pós*, mostrado abaixo, é responsável por transferir as entradas especificadas em seu argumento para o vetor de entradas da rede neural. O processo é feito iterativamente em um loop *for*, visto que caso o procesos fosse realizado por atribuição direta a referência entre o vetor de entradas da rede neural e o vetor de entradas da primeira camada seria perdida.\n",
    "\n",
    "~~~~python\n",
    "def set_entradas_pós(self, entradas, normalizar):\n",
    "    if normalizar:           \n",
    "        vetorEntradas = self.normalizar(entradas, True)     \n",
    "        for i in range(len(vetorEntradas)):\n",
    "            self.entradas[i] = vetorEntradas[i]\n",
    "    else:\n",
    "        for i in range(len(entradas)):\n",
    "            self.entradas[i] = entradas[i]\n",
    "~~~~\n",
    "\n",
    "A inserção de dados na rede neural deve ser realizada através do vetor *listaEntradas*, e os dados já devem estar normalizados. Caso não estejam, a classe *RedeNeural* possui o método *normalizar*, exatamente igual ao método implementado nos neurônios.\n",
    "\n",
    "**Exemplo**\n",
    "\n",
    "O código abaixo implementa uma rede neural com 2 entradas, 2 camadas ocultas com 5 neurônios em cada camada e 2 saídas. A função de ativação é sigmoidal. Como entrada, serão utilizados os dados apresentados na tabela abaixo. Como os dados não estão normalizados será necessário normalizá-los.\n",
    "![Tabela](https://i.imgur.com/2PxXV7U.png)\n",
    "\n",
    "~~~~python\n",
    "rede = RedeNeural(2, 2, 5, 2, 'Sigmoid')\n",
    "Entradas = np.array([[0,0],[0.5,0.3],[0.1,0.6],[1,2]])\n",
    "rede.listaEntradas = rede.normalizar(Entradas, False)\n",
    "~~~~\n",
    "\n",
    "O parâmetro False na função normalizar faz com que seja definida uma nova média e desvio padrão a partir das entradas fornecidas para o processo de normalização.\n",
    "\n",
    "**Processamento das saídas**\n",
    "\n",
    "Para o processamento de saídas foram implementados dois métodos. O primeiro, denominado *proc_saidas*, processa as saídas da rede neural para todas as entradas presentes no vetor *listaEntradas*. O outro método, denominado *proc_saida*, processa as saídas da rede neural para somente uma entrada presente no vetor *listaEntradas*, especificada pelo parâmetro indice. O processamento das saídas é realizado da seguinte forma:\n",
    "1. Remove os elementos já processados no vetor *listaSaidas*;\n",
    "1. Configura o vetor *entradas* com o elemento escolhido do vetor *listaEntradas*;\n",
    "1. Percorre as camadas, da primeira à última, processando suas saídas pelo método *Camada.proc_saida*. Como suas entradas e saídas estão interligadas basta chamar o método seguindo as ordens das camadas, que suas entradas serão atualizadas automaticamente no processamento da saída da camada anterior;\n",
    "1. Adiciona a saída processada ao vetor *listaSaidas*.\n",
    "\n",
    "**Algoritmo de treinamento**\n",
    "\n",
    "O algoritmo de treinamento recebe como parâmetros:\n",
    "1. Vetor de saídas desejadas;\n",
    "1. Taxa de aprendizado inicial $\\eta$;\n",
    "1. Taxa de *annealing*;\n",
    "1. Taxa de momento;\n",
    "1. Número de iterações a serem executadas.\n",
    "\n",
    "O processo de treinamento é realizado da seguinte forma:\n",
    "\n",
    "1. Seleciona-se um conjunto de entradas para a rede neural e encontra-se as saídas $y(k)$, onde $k$ representa a última camada da rede neural, para a entrada fornecida.\n",
    "1. Inicialmente atua-se na última camada:\n",
    "  1. Com as saídas $y(k)$ já calculadas para cada neurônio da camada de saída, calcula-se o $\\delta_k(n) = y(k)(1-y(k))(d(k)-y(k))$ para cada neurônio desta camada. O valor de $\\delta_k$ é armazenado na variável *Neuronio.delta*.\n",
    "  1. Calcula-se o momento para ajuste dos pesos de cada neurônio, dado por $m = \\alpha(w(n)-w(n-1))$, onde $\\alpha$ corresponde à taxa de momento especificada no argumento da função de treinamento.\n",
    "  1. Para cada neurônio, $n.novosPesos = w(n) + \\eta\\delta_k y(k-1) + m$\n",
    "    1. Obs: os pesos ainda não são atualizados pois os pesos das camadas anteriores ainda devem ser ajustados\n",
    "1. Seguindo agora da penúltima camada para trás, sendo a camada corrente $i$ e a camada seguinte $j$:\n",
    "  1. Para cada neurônio $n$ da camada $i$, realiza-se o somatório de $w_{jn}\\delta_j$ para cada neurônio da camada $j$. Este somatório é armazenado na variável *soma*.\n",
    "  1. O $\\delta_i$ para o neurônio $n$ é então calculado por $\\delta_i = y(n)(1-y(n))soma$\n",
    "  1. Calcula-se o momento para cada neurônio da camada $i$\n",
    "  1. Define-se os novos pesos para os neurônios da camada $i$\n",
    "1. Troca-se os pesos dos neurônios chamando a função *Neuronio.trocarPesos* em cada neurônio\n",
    "\n",
    "Em seguida o processo é repetido para cada conjunto de entradas no vetor *listaEntradas*, e repetido novamente pelo número de iterações definidos no argumento da função de treinamento. Para cada iteração, o valor da taxa de aprendizado $\\eta$ é calculado pela equação:\n",
    "\n",
    "$\\eta = \\frac{\\eta_0}{1+\\frac{n}{\\tau}}$\n",
    "\n",
    "Onde $\\eta_0$ corresponde à taxa de aprendizado inicial, $n$ corresponde à iteração corrente e $\\tau$ corresponde à taxa de *annealing*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Implementação da função XOR utilizando a rede MLP\n",
    "\n",
    "Inicialmente instancia-se a rede neural. Será utilizada uma rede com 2 entradas, 1 camada de entrada, 1 camada de saída e função de ativação sigmoidal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "redeXOR = RedeNeural(2, 0, 0, 1, 'Sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As entradas e saídas desejadas utilizadas para treinamento serão definidas conforme apresentado na tabela abaixo. Será adicionado um \"ruído\" em torno dos valores de entrada para ajudar no processo de treinamento.\n",
    "\n",
    "![XOR](https://i.imgur.com/wg4Ckhs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criação do array de entradas\n",
    "entradas = np.zeros([100,2])\n",
    "ruido = np.transpose([np.array(np.random.normal(scale=0.03, size=50))])\n",
    "entradas[:50,0] = entradas[:50,0] + ruido[:,0]\n",
    "ruido = np.transpose([np.array(np.random.normal(scale=0.03, size=50))])\n",
    "entradas[50:,0] = entradas[50:,0] + ruido[:,0] + 1\n",
    "ruido = np.transpose([np.array(np.random.normal(scale=0.03, size=25))])\n",
    "entradas[:25,1] = entradas[:25,1] + ruido[:,0]\n",
    "ruido = np.transpose([np.array(np.random.normal(scale=0.03, size=25))])\n",
    "entradas[25:50,1] = entradas[25:50,1] + ruido[:,0] + 1\n",
    "ruido = np.transpose([np.array(np.random.normal(scale=0.03, size=25))])\n",
    "entradas[50:75,1] = entradas[50:75,1] + ruido[:,0]\n",
    "ruido = np.transpose([np.array(np.random.normal(scale=0.03, size=25))])\n",
    "entradas[75:100,1] = entradas[75:100,1] + ruido[:,0] + 1\n",
    "\n",
    "\n",
    "saidas_desejadas = np.zeros([100,1])\n",
    "saidas_desejadas[25:75,0] += 1\n",
    "ruido = np.transpose([np.array(np.random.normal(scale=0.03, size=100))])\n",
    "saidas_desejadas += ruido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida configura-se as entradas da rede neural para o vetor *entradas*. Como as entradas não estão normalizadas deve-se executar o método *normalizar*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.94798291 -0.96837509]\n",
      " [-0.96811546 -1.01147191]\n",
      " [-1.16989994 -1.02084994]\n",
      " [-1.02181351 -0.84227663]\n",
      " [-1.09572594 -0.91370583]\n",
      " [-0.88043681 -0.91180564]\n",
      " [-1.05016097 -0.96810861]\n",
      " [-1.01168618 -1.0321266 ]\n",
      " [-1.05542923 -1.01085166]\n",
      " [-1.02617254 -1.00718015]\n",
      " [-1.06285677 -1.02940555]\n",
      " [-0.83418964 -0.97911252]\n",
      " [-1.05118522 -0.97022703]\n",
      " [-0.95401017 -0.97510407]\n",
      " [-0.98487135 -1.04864824]\n",
      " [-1.00983782 -1.00236428]\n",
      " [-1.02649018 -0.96351634]\n",
      " [-1.07209824 -1.00745756]\n",
      " [-1.03777584 -0.99049997]\n",
      " [-0.94932301 -1.03318573]\n",
      " [-1.07021298 -1.10682443]\n",
      " [-1.06365913 -1.02455347]\n",
      " [-1.09404019 -0.95929128]\n",
      " [-1.0855332  -1.03506293]\n",
      " [-1.00309995 -0.95393699]\n",
      " [-1.08478012  1.09144161]\n",
      " [-1.05053807  0.99704513]\n",
      " [-1.03711281  1.03970444]\n",
      " [-0.91473443  0.99872504]\n",
      " [-0.90037112  0.949166  ]\n",
      " [-0.91046472  1.09223557]\n",
      " [-1.01169472  0.94414148]\n",
      " [-0.91091653  1.09315862]\n",
      " [-0.97332313  1.01601717]\n",
      " [-1.01042335  0.9818319 ]\n",
      " [-0.9281554   0.97594947]\n",
      " [-0.99059576  1.06052616]\n",
      " [-0.96179317  1.01655864]\n",
      " [-0.9861368   0.91941527]\n",
      " [-0.9153522   0.95410534]\n",
      " [-0.99558476  1.03066791]\n",
      " [-0.99644183  1.02000455]\n",
      " [-1.04845393  0.98695406]\n",
      " [-0.92055029  0.99804294]\n",
      " [-1.06629402  0.97288007]\n",
      " [-1.01537837  0.98268609]\n",
      " [-0.91715371  1.03353696]\n",
      " [-0.91516768  0.9997738 ]\n",
      " [-1.04776888  1.01126999]\n",
      " [-0.86314966  1.01187462]\n",
      " [ 1.0493184  -1.02926786]\n",
      " [ 0.91306937 -1.04195137]\n",
      " [ 0.96558985 -1.02342399]\n",
      " [ 1.08906313 -1.03696411]\n",
      " [ 0.99774333 -0.95018463]\n",
      " [ 0.91478929 -1.06984307]\n",
      " [ 1.03313986 -0.92855536]\n",
      " [ 1.01599601 -1.06462586]\n",
      " [ 1.06880839 -0.94962574]\n",
      " [ 0.97084274 -0.96551511]\n",
      " [ 0.90148237 -1.05944991]\n",
      " [ 1.03250783 -1.036996  ]\n",
      " [ 1.03624706 -1.06901139]\n",
      " [ 1.00259793 -0.98379604]\n",
      " [ 0.99115378 -1.00510165]\n",
      " [ 1.04551027 -0.96943558]\n",
      " [ 1.05253199 -0.98622986]\n",
      " [ 1.05447606 -0.91107064]\n",
      " [ 0.97791077 -1.08651775]\n",
      " [ 0.87458684 -1.01100858]\n",
      " [ 1.01103769 -0.91211244]\n",
      " [ 0.96043303 -1.02189581]\n",
      " [ 0.94011681 -0.95538329]\n",
      " [ 0.95754706 -1.05184055]\n",
      " [ 1.00644521 -1.05047942]\n",
      " [ 0.98787379  1.05794973]\n",
      " [ 1.00659153  1.02560823]\n",
      " [ 0.9797691   0.97458866]\n",
      " [ 1.07473987  0.9924652 ]\n",
      " [ 1.01870873  1.00293827]\n",
      " [ 1.00831141  1.00153598]\n",
      " [ 0.93017995  0.89236622]\n",
      " [ 0.97991615  0.97100931]\n",
      " [ 1.09601095  1.07281687]\n",
      " [ 1.00481919  1.04066792]\n",
      " [ 0.83592341  1.02128188]\n",
      " [ 1.04455516  0.97712056]\n",
      " [ 0.97876085  0.86805144]\n",
      " [ 0.98399931  1.08121495]\n",
      " [ 0.98209356  0.9509382 ]\n",
      " [ 0.98324882  0.99387861]\n",
      " [ 1.01896936  1.03364298]\n",
      " [ 1.02291583  1.00769981]\n",
      " [ 1.12079294  1.01679651]\n",
      " [ 1.0179923   0.96844417]\n",
      " [ 1.02452837  0.92329863]\n",
      " [ 0.98407574  0.93311285]\n",
      " [ 0.99433424  0.98446807]\n",
      " [ 1.06927442  0.93941987]\n",
      " [ 0.88761259  1.02720071]]\n"
     ]
    }
   ],
   "source": [
    "redeXOR.listaEntradas = redeXOR.normalizar(entradas, False)\n",
    "print(redeXOR.listaEntradas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com as entradas definidas, é realizado então o treinamento da rede. Para o treinamento, foi escolhida uma taxa de aprendizado $\\eta = 30$, *annealing* igual a 600, taxa de momento de 0,1 e 2000 iterações. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#redeXOR.treinar_rede(saidas_desejadas, 30, 600, 0.1, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "eta = 20.0 ; m=0\n",
      "[[-0.94798291 -0.96837509]] - [[[0.53448493]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.00685245]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.93352283]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.98486195]]]\n",
      "-----------------------------------\n",
      "eta = 19.354838709677416 ; m=1\n",
      "[[-0.94798291 -0.96837509]] - [[[0.05573842]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.05862353]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.71591174]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.99236839]]]\n",
      "-----------------------------------\n",
      "eta = 18.75 ; m=2\n",
      "[[-0.94798291 -0.96837509]] - [[[0.46917984]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.08532552]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.12548861]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.98952595]]]\n",
      "-----------------------------------\n",
      "eta = 18.18181818181818 ; m=3\n",
      "[[-0.94798291 -0.96837509]] - [[[0.56157193]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.14393292]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.01036169]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.99935019]]]\n",
      "-----------------------------------\n",
      "eta = 17.647058823529413 ; m=4\n",
      "[[-0.94798291 -0.96837509]] - [[[0.99943174]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99912112]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99803554]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.99573386]]]\n",
      "-----------------------------------\n",
      "eta = 17.142857142857142 ; m=5\n",
      "[[-0.94798291 -0.96837509]] - [[[0.12762021]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.87056365]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.06379814]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.97221428]]]\n",
      "-----------------------------------\n",
      "eta = 16.666666666666668 ; m=6\n",
      "[[-0.94798291 -0.96837509]] - [[[0.135844]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.81897781]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.11231565]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.33089194]]]\n",
      "-----------------------------------\n",
      "eta = 16.216216216216214 ; m=7\n",
      "[[-0.94798291 -0.96837509]] - [[[0.71468931]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.76582205]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.01132889]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.80024228]]]\n",
      "-----------------------------------\n",
      "eta = 15.789473684210527 ; m=8\n",
      "[[-0.94798291 -0.96837509]] - [[[0.35280673]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.96891676]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.12248627]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.17362216]]]\n",
      "-----------------------------------\n",
      "eta = 15.384615384615383 ; m=9\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01514678]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99311177]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.93417515]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.04103502]]]\n",
      "-----------------------------------\n",
      "eta = 15.0 ; m=10\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01802577]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99360322]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.96148482]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.03109326]]]\n",
      "-----------------------------------\n",
      "eta = 14.634146341463415 ; m=11\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01767278]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99363067]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.97038855]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.0257711]]]\n",
      "-----------------------------------\n",
      "eta = 14.285714285714286 ; m=12\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01698328]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99361913]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.97542528]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.02247914]]]\n",
      "-----------------------------------\n",
      "eta = 13.953488372093023 ; m=13\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01627249]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99360232]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.97870421]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.0202282]]]\n",
      "-----------------------------------\n",
      "eta = 13.636363636363635 ; m=14\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01561939]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99358924]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.9810267]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.01858266]]]\n",
      "-----------------------------------\n",
      "eta = 13.333333333333334 ; m=15\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01503908]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99358225]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.98276863]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.01732124]]]\n",
      "-----------------------------------\n",
      "eta = 13.043478260869566 ; m=16\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01452785]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99358147]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.98413022]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.01631947]]]\n",
      "-----------------------------------\n",
      "eta = 12.76595744680851 ; m=17\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01407719]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99358631]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.98522813]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.01550188]]]\n",
      "-----------------------------------\n",
      "eta = 12.5 ; m=18\n",
      "[[-0.94798291 -0.96837509]] - [[[0.0136782]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99359594]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.98613515]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.01481998]]]\n",
      "-----------------------------------\n",
      "eta = 12.244897959183673 ; m=19\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01332297]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99360955]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.98689913]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.01424112]]]\n",
      "-----------------------------------\n",
      "eta = 12.000000000000002 ; m=20\n",
      "[[-0.94798291 -0.96837509]] - [[[0.0130048]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99362641]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.98755289]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.0137425]]]\n",
      "-----------------------------------\n",
      "eta = 11.764705882352942 ; m=21\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01271813]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99364589]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.98811975]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.01330769]]]\n",
      "-----------------------------------\n",
      "eta = 11.538461538461538 ; m=22\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01245841]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99366745]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.98861677]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.01292452]]]\n",
      "-----------------------------------\n",
      "eta = 11.320754716981133 ; m=23\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01222187]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99369065]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.98905672]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.0125838]]]\n",
      "-----------------------------------\n",
      "eta = 11.11111111111111 ; m=24\n",
      "[[-0.94798291 -0.96837509]] - [[[0.0120054]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.9937151]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.98944937]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.01227843]]]\n",
      "-----------------------------------\n",
      "eta = 10.909090909090908 ; m=25\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01180642]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99374051]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.98980234]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.01200285]]]\n",
      "-----------------------------------\n",
      "eta = 10.714285714285714 ; m=26\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01162278]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99376662]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99012166]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.01175263]]]\n",
      "-----------------------------------\n",
      "eta = 10.526315789473685 ; m=27\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01145264]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99379322]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99041216]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.01152419]]]\n",
      "-----------------------------------\n",
      "eta = 10.344827586206897 ; m=28\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01129448]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99382014]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99067779]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.01131461]]]\n",
      "-----------------------------------\n",
      "eta = 10.169491525423728 ; m=29\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01114699]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99384724]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99092178]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.0111215]]]\n",
      "-----------------------------------\n",
      "eta = 10.0 ; m=30\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01100903]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99387441]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.9911468]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.01094285]]]\n",
      "-----------------------------------\n",
      "eta = 9.836065573770492 ; m=31\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01087965]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99390154]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99135511]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.01077698]]]\n",
      "-----------------------------------\n",
      "eta = 9.67741935483871 ; m=32\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01075801]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99392857]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.9915486]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.01062247]]]\n",
      "-----------------------------------\n",
      "eta = 9.523809523809524 ; m=33\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01064337]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99395542]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99172888]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.01047809]]]\n",
      "-----------------------------------\n",
      "eta = 9.375 ; m=34\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01053509]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99398206]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99189734]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.01034281]]]\n",
      "-----------------------------------\n",
      "eta = 9.23076923076923 ; m=35\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01043262]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99400843]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99205517]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.01021573]]]\n",
      "-----------------------------------\n",
      "eta = 9.09090909090909 ; m=36\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01033547]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99403451]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99220341]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.01009606]]]\n",
      "-----------------------------------\n",
      "eta = 8.955223880597014 ; m=37\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01024318]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99406027]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99234294]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00998313]]]\n",
      "-----------------------------------\n",
      "eta = 8.823529411764707 ; m=38\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01015538]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99408568]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99247457]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00987634]]]\n",
      "-----------------------------------\n",
      "eta = 8.695652173913045 ; m=39\n",
      "[[-0.94798291 -0.96837509]] - [[[0.01007172]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99411075]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99259897]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00977515]]]\n",
      "-----------------------------------\n",
      "eta = 8.571428571428573 ; m=40\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00999187]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99413545]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99271677]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00967911]]]\n",
      "-----------------------------------\n",
      "eta = 8.450704225352112 ; m=41\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00991557]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99415977]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.0493184  -1.02926786]] - [[[0.99282852]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.0095878]]]\n",
      "-----------------------------------\n",
      "eta = 8.333333333333334 ; m=42\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00984256]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99418372]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99293468]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00950084]]]\n",
      "-----------------------------------\n",
      "eta = 8.21917808219178 ; m=43\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00977261]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99420728]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.9930357]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00941791]]]\n",
      "-----------------------------------\n",
      "eta = 8.108108108108107 ; m=44\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00970552]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99423047]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99313197]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00933871]]]\n",
      "-----------------------------------\n",
      "eta = 8.0 ; m=45\n",
      "[[-0.94798291 -0.96837509]] - [[[0.0096411]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99425327]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99322382]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00926297]]]\n",
      "-----------------------------------\n",
      "eta = 7.894736842105264 ; m=46\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00957917]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99427569]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99331159]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00919045]]]\n",
      "-----------------------------------\n",
      "eta = 7.792207792207793 ; m=47\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00951958]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99429774]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99339554]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00912093]]]\n",
      "-----------------------------------\n",
      "eta = 7.692307692307692 ; m=48\n",
      "[[-0.94798291 -0.96837509]] - [[[0.0094622]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99431941]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99347595]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00905422]]]\n",
      "-----------------------------------\n",
      "eta = 7.59493670886076 ; m=49\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00940687]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99434072]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99355304]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00899013]]]\n",
      "-----------------------------------\n",
      "eta = 7.499999999999999 ; m=50\n",
      "[[-0.94798291 -0.96837509]] - [[[0.0093535]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99436167]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99362703]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00892849]]]\n",
      "-----------------------------------\n",
      "eta = 7.4074074074074066 ; m=51\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00930196]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99438226]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99369811]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00886916]]]\n",
      "-----------------------------------\n",
      "eta = 7.317073170731708 ; m=52\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00925215]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.9944025]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99376647]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.008812]]]\n",
      "-----------------------------------\n",
      "eta = 7.228915662650603 ; m=53\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00920398]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99442239]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99383227]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00875687]]]\n",
      "-----------------------------------\n",
      "eta = 7.142857142857143 ; m=54\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00915737]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99444195]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99389565]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00870367]]]\n",
      "-----------------------------------\n",
      "eta = 7.058823529411765 ; m=55\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00911222]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99446117]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99395676]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00865228]]]\n",
      "-----------------------------------\n",
      "eta = 6.976744186046512 ; m=56\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00906847]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99448008]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99401573]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00860261]]]\n",
      "-----------------------------------\n",
      "eta = 6.8965517241379315 ; m=57\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00902604]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99449866]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99407267]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00855455]]]\n",
      "-----------------------------------\n",
      "eta = 6.8181818181818175 ; m=58\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00898487]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99451693]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99412769]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00850804]]]\n",
      "-----------------------------------\n",
      "eta = 6.741573033707865 ; m=59\n",
      "[[-0.94798291 -0.96837509]] - [[[0.0089449]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.9945349]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.9941809]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00846298]]]\n",
      "-----------------------------------\n",
      "eta = 6.666666666666667 ; m=60\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00890607]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99455257]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99423239]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.0084193]]]\n",
      "-----------------------------------\n",
      "eta = 6.593406593406594 ; m=61\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00886833]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99456996]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99428225]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00837694]]]\n",
      "-----------------------------------\n",
      "eta = 6.521739130434782 ; m=62\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00883163]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99458705]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99433055]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00833583]]]\n",
      "-----------------------------------\n",
      "eta = 6.451612903225806 ; m=63\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00879592]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99460387]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99437739]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00829591]]]\n",
      "-----------------------------------\n",
      "eta = 6.382978723404255 ; m=64\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00876116]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99462042]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99442282]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00825713]]]\n",
      "-----------------------------------\n",
      "eta = 6.315789473684211 ; m=65\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00872731]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.9946367]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99446691]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00821943]]]\n",
      "-----------------------------------\n",
      "eta = 6.25 ; m=66\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00869433]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99465272]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99450973]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00818276]]]\n",
      "-----------------------------------\n",
      "eta = 6.185567010309279 ; m=67\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00866218]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99466849]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99455134]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00814708]]]\n",
      "-----------------------------------\n",
      "eta = 6.122448979591836 ; m=68\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00863082]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99468401]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99459178]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00811234]]]\n",
      "-----------------------------------\n",
      "eta = 6.0606060606060606 ; m=69\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00860024]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99469929]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99463111]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00807851]]]\n",
      "-----------------------------------\n",
      "eta = 6.0 ; m=70\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00857038]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99471433]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99466939]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00804554]]]\n",
      "-----------------------------------\n",
      "eta = 5.9405940594059405 ; m=71\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00854124]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99472914]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99470664]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.0080134]]]\n",
      "-----------------------------------\n",
      "eta = 5.882352941176471 ; m=72\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00851277]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99474373]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99474293]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00798205]]]\n",
      "-----------------------------------\n",
      "eta = 5.825242718446602 ; m=73\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00848496]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.9947581]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99477828]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00795147]]]\n",
      "-----------------------------------\n",
      "eta = 5.769230769230769 ; m=74\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00845777]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99477225]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99481274]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00792162]]]\n",
      "-----------------------------------\n",
      "eta = 5.714285714285714 ; m=75\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00843119]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99478619]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99484634]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00789248]]]\n",
      "-----------------------------------\n",
      "eta = 5.660377358490567 ; m=76\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00840519]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99479992]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99487912]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00786401]]]\n",
      "-----------------------------------\n",
      "eta = 5.607476635514018 ; m=77\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00837975]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99481346]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.9949111]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00783619]]]\n",
      "-----------------------------------\n",
      "eta = 5.555555555555555 ; m=78\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00835486]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99482679]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99494233]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.007809]]]\n",
      "-----------------------------------\n",
      "eta = 5.504587155963303 ; m=79\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00833049]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99483994]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99497282]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00778242]]]\n",
      "-----------------------------------\n",
      "eta = 5.454545454545455 ; m=80\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00830662]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.9948529]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.9950026]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00775641]]]\n",
      "-----------------------------------\n",
      "eta = 5.405405405405405 ; m=81\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00828325]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99486567]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99503171]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00773097]]]\n",
      "-----------------------------------\n",
      "eta = 5.357142857142857 ; m=82\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00826034]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99487827]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99506016]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00770607]]]\n",
      "-----------------------------------\n",
      "eta = 5.3097345132743365 ; m=83\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00823789]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99489068]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99508799]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00768169]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "eta = 5.2631578947368425 ; m=84\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00821588]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99490293]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.9951152]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00765782]]]\n",
      "-----------------------------------\n",
      "eta = 5.217391304347826 ; m=85\n",
      "[[-0.94798291 -0.96837509]] - [[[0.0081943]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99491501]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99514183]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00763443]]]\n",
      "-----------------------------------\n",
      "eta = 5.172413793103448 ; m=86\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00817313]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99492693]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99516789]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00761151]]]\n",
      "-----------------------------------\n",
      "eta = 5.128205128205129 ; m=87\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00815236]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99493868]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.9951934]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00758905]]]\n",
      "-----------------------------------\n",
      "eta = 5.084745762711865 ; m=88\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00813198]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99495028]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99521839]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00756703]]]\n",
      "-----------------------------------\n",
      "eta = 5.042016806722689 ; m=89\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00811198]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99496172]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99524286]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00754544]]]\n",
      "-----------------------------------\n",
      "eta = 5.0 ; m=90\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00809233]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99497301]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99526684]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00752425]]]\n",
      "-----------------------------------\n",
      "eta = 4.958677685950414 ; m=91\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00807305]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99498416]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99529034]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00750347]]]\n",
      "-----------------------------------\n",
      "eta = 4.918032786885246 ; m=92\n",
      "[[-0.94798291 -0.96837509]] - [[[0.0080541]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99499516]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99531338]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00748308]]]\n",
      "-----------------------------------\n",
      "eta = 4.878048780487806 ; m=93\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00803549]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99500602]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99533597]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00746306]]]\n",
      "-----------------------------------\n",
      "eta = 4.838709677419355 ; m=94\n",
      "[[-0.94798291 -0.96837509]] - [[[0.0080172]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99501674]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99535812]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.0074434]]]\n",
      "-----------------------------------\n",
      "eta = 4.800000000000001 ; m=95\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00799923]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99502732]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99537986]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.0074241]]]\n",
      "-----------------------------------\n",
      "eta = 4.761904761904762 ; m=96\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00798156]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99503778]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99540118]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00740514]]]\n",
      "-----------------------------------\n",
      "eta = 4.724409448818897 ; m=97\n",
      "[[-0.94798291 -0.96837509]] - [[[0.00796419]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.9950481]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99542212]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00738652]]]\n",
      "-----------------------------------\n",
      "eta = 4.6875 ; m=98\n",
      "[[-0.94798291 -0.96837509]] - [[[0.0079471]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99505829]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99544266]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00736821]]]\n",
      "-----------------------------------\n",
      "eta = 4.651162790697675 ; m=99\n",
      "[[-0.94798291 -0.96837509]] - [[[0.0079303]]]\n",
      "[[-1.08478012  1.09144161]] - [[[0.99506836]]]\n",
      "[[ 1.0493184  -1.02926786]] - [[[0.99546284]]]\n",
      "[[0.98787379 1.05794973]] - [[[0.00735023]]]\n"
     ]
    }
   ],
   "source": [
    "redeXOR = RedeNeural(2, 0, 0, 1, 'Sigmoid')\n",
    "redeXOR.listaEntradas = redeXOR.normalizar(entradas, False)\n",
    "redeXOR.treinar_rede(saidas_desejadas, 20, 30, 0.1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a rede neural treinada, agora será gerado um novo conjunto de dados para validação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criação do array de entradas de validação\n",
    "entradasV = np.zeros([100,2])\n",
    "ruido = np.transpose([np.array(np.random.normal(scale=0.03, size=50))])\n",
    "entradasV[:50,0] = entradas[:50,0] + ruido[:,0]\n",
    "ruido = np.transpose([np.array(np.random.normal(scale=0.03, size=50))])\n",
    "entradasV[50:,0] = entradas[50:,0] + ruido[:,0] + 1\n",
    "ruido = np.transpose([np.array(np.random.normal(scale=0.03, size=25))])\n",
    "entradasV[:25,1] = entradas[:25,1] + ruido[:,0]\n",
    "ruido = np.transpose([np.array(np.random.normal(scale=0.03, size=25))])\n",
    "entradasV[25:50,1] = entradas[25:50,1] + ruido[:,0] + 1\n",
    "ruido = np.transpose([np.array(np.random.normal(scale=0.03, size=25))])\n",
    "entradasV[50:75,1] = entradas[50:75,1] + ruido[:,0]\n",
    "ruido = np.transpose([np.array(np.random.normal(scale=0.03, size=25))])\n",
    "entradasV[75:100,1] = entradas[75:100,1] + ruido[:,0] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a criação dos dados é feita a validação da rede:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "redeXOR.listaEntradas = redeXOR.normalizar(entradasV, True)\n",
    "redeXOR.proc_saidas(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Entradas] -> [Saída]\n",
      "[0, 0] -> 0\n",
      "[0, 0] -> 0\n",
      "[0, 0] -> 0\n",
      "[0, 0] -> 0\n",
      "[0, 0] -> 0\n",
      "[0, 0] -> 0\n",
      "[0, 0] -> 0\n",
      "[0, 0] -> 0\n",
      "[0, 0] -> 0\n",
      "[0, 0] -> 0\n",
      "[0, 0] -> 0\n",
      "[0, 0] -> 0\n",
      "[0, 0] -> 0\n",
      "[0, 0] -> 0\n",
      "[0, 0] -> 0\n",
      "[0, 0] -> 0\n",
      "[0, 0] -> 0\n",
      "[0, 0] -> 0\n",
      "[0, 0] -> 0\n",
      "[0, 0] -> 0\n",
      "[0, 0] -> 0\n",
      "[0, 0] -> 0\n",
      "[0, 0] -> 0\n",
      "[0, 0] -> 0\n",
      "[0, 0] -> 0\n",
      "[0, 1] -> 1\n",
      "[0, 1] -> 1\n",
      "[0, 1] -> 1\n",
      "[0, 1] -> 1\n",
      "[0, 1] -> 1\n",
      "[0, 1] -> 1\n",
      "[0, 1] -> 1\n",
      "[0, 1] -> 1\n",
      "[0, 1] -> 1\n",
      "[0, 1] -> 1\n",
      "[0, 1] -> 1\n",
      "[0, 1] -> 1\n",
      "[0, 1] -> 1\n",
      "[0, 1] -> 1\n",
      "[0, 1] -> 1\n",
      "[0, 1] -> 1\n",
      "[0, 1] -> 1\n",
      "[0, 1] -> 1\n",
      "[0, 1] -> 1\n",
      "[0, 1] -> 1\n",
      "[0, 1] -> 1\n",
      "[0, 1] -> 1\n",
      "[0, 1] -> 1\n",
      "[0, 1] -> 1\n",
      "[0, 1] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 0] -> 1\n",
      "[1, 1] -> 0\n",
      "[1, 1] -> 0\n",
      "[1, 1] -> 0\n",
      "[1, 1] -> 0\n",
      "[1, 1] -> 0\n",
      "[1, 1] -> 0\n",
      "[1, 1] -> 0\n",
      "[1, 1] -> 0\n",
      "[1, 1] -> 0\n",
      "[1, 1] -> 0\n",
      "[1, 1] -> 0\n",
      "[1, 1] -> 0\n",
      "[1, 1] -> 0\n",
      "[1, 1] -> 0\n",
      "[1, 1] -> 0\n",
      "[1, 1] -> 0\n",
      "[1, 1] -> 0\n",
      "[1, 1] -> 0\n",
      "[1, 1] -> 0\n",
      "[1, 1] -> 0\n",
      "[1, 1] -> 0\n",
      "[1, 1] -> 0\n",
      "[1, 1] -> 0\n",
      "[1, 1] -> 0\n",
      "[1, 1] -> 0\n",
      "Nº total de erros = 0\n"
     ]
    }
   ],
   "source": [
    "erros = 0\n",
    "print('[Entradas] -> [Saída]')\n",
    "for i, c in enumerate(redeXOR.listaSaidas):\n",
    "    if i<25:\n",
    "        print('[0, 0] -> {}'.format(int(c[0,0]+0.5)))\n",
    "        if int(c[0,0]+0.5) != 0:\n",
    "            erros += 1\n",
    "    elif i>=25 and i<50:\n",
    "        print('[0, 1] -> {}'.format(int(c[0,0]+0.5)))\n",
    "        if int(c[0,0]+0.5) != 1:\n",
    "            erros += 1\n",
    "    elif i>=50 and i<75:\n",
    "        print('[1, 0] -> {}'.format(int(c[0,0]+0.5)))\n",
    "        if int(c[0,0]+0.5) != 1:\n",
    "            erros += 1\n",
    "    else:\n",
    "        print('[1, 1] -> {}'.format(int(c[0,0]+0.5)))\n",
    "        if int(c[0,0]+0.5) != 0:\n",
    "            erros += 1\n",
    "print('Nº total de erros = {}'.format(erros))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebe-se que a rede neural consegue aproximar uma função XOR. Porém, o sucesso desta aproximação depende muito da escolha dos pesos iniciais. A escolha de  um certo conjunto de pesos leva à uma ótima aproximação, mas caso os pesos escolhidos inicialmente não sejam apropriados a aproximação não ocorre, levando à ocorrência de erros durante o processo de validação. Como a escolha dos pesos é aleatória, pode ser necessário realizar o treinamento mais de uma vez até alcançar um resultado ótimo.\n",
    "\n",
    "## Análise de desempenho do processo de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.94798291],\n",
      "       [-0.96837509]]), array([[0.53448493]])]\n"
     ]
    }
   ],
   "source": [
    "print(redeXOR.registroTreinamento[0])\n",
    "#print(len(redeXOR.registroTreinamento[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "listaErros = []\n",
    "#print(redeXOR.registroTreinamento[1][1][0][0])\n",
    "for i in range(int(len(redeXOR.registroTreinamento)/100)):\n",
    "    erros=[]\n",
    "    for x in range(4):\n",
    "        if x==0 or x==3:            \n",
    "            erros.append(redeXOR.registroTreinamento[x*25+i*100][1][0][0])\n",
    "        else:\n",
    "            erros.append(redeXOR.registroTreinamento[x*25+i*100][1][0][0] - 1)\n",
    "    listaErros.append(np.mean(np.abs(erros)))\n",
    "\n",
    "plt.plot(listaErros)\n",
    "plt.xlabel('Iteração')\n",
    "plt.ylabel('Erro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebe-se uma queda brusca no valor de erro em um espaço de tempo muito pequeno. Isto se deve possivelmente à função de ativação sigmoidal utilizada nos neurônios, juntamente com o fato de a taxa de aprendizagem ser relativamente alta.\n",
    "\n",
    "# 4 Conclusão\n",
    "\n",
    "É possível concluir que as redes neurais de múltiplas camadas conseguem aproximar funções que não são linearmente separáveis. O algoritmo de treinamento por *backpropagation* foi bastante efetivo para minimizar o erro, necessitando de apenas 10 iterações para alcançar o mínimo global. Foi observado que durante alguns processos de treinamento não houve redução do erro. Isto ocorreu devido à definição dos pesos inciais, demonstrando assim que os pesos iniciais devem ser considerados durante o processo de treinamento de uma rede neural.\n",
    "\n",
    "Houve a necessidade de gerar mais amostras para o treinamento da rede neural, visto que o ajuste dos pesos utilizando poucas amostras para treinamento se mostrou bastante irregular, levando à erros na saída da rede frequentemente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
